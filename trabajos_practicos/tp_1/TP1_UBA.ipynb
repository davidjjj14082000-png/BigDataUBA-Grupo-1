{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Dh8MkXaG-c9Y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Big Data y Machine Learning (UBA) -  2025\n",
    "\n",
    "## Trabajo Práctico 1: Jugando con APIs y WebScraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhBlm6mZ-c9e"
   },
   "source": [
    "### Reglas de formato y presentación\n",
    "- El trabajo debe estar debidamente documentado comentado (utilizando #) para que tanto los docentes como sus compañeros puedan comprender el código fácilmente.\n",
    "\n",
    "- El mismo debe ser completado en este Jupyter Notebook y entregado como tal, es decir en un archivo .ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEjGaa4U-c9g"
   },
   "source": [
    "### Fecha de entrega:\n",
    "<font color=red>Viernes 5 de Septiembre a las 13:00 hs</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9TU2y7E-c9h"
   },
   "source": [
    "### Modalidad de entrega\n",
    "- Al finalizar el trabajo práctico deben hacer un último <i>commit</i> en su repositorio de GitHub llamado “Entrega final del tp”. \n",
    "- Asegurense de haber creado una carpeta llamada TP1. Este Jupyter Notebook y el correspondiente al TP1 deben estar dentro de esa carpeta.\n",
    "- También deben enviar el link de su repositorio -para que pueda ser clonado y corregido- a mi correo 25RO35480961@campus.economicas.uba.ar. Usar de asunto de email <i>\"Big Data - TP 1 - Grupo #\"</i> y nombrar el archivo <i>\"TP1_Grupo #\"</i> donde # es el número de grupo que le fue asignado.\n",
    "- La última versión en el repositorio es la que será evaluada. Por lo que es importante que: \n",
    "    - No envien el correo hasta no haber terminado y estar seguros de que han hecho el <i>commit y push</i> a la versión final que quieren entregar. \n",
    "    - No hagan nuevos <i>push</i> despues de haber entregado su versión final. Esto generaría confusión acerca de que versión es la que quieren que se les corrija.\n",
    "- En resumen, la carpeta del repositorio debe incluir:\n",
    "    - El codigo\n",
    "    - Un documento Word (Parte A) donde esten las figuras y una breve descripción de las mismas.\n",
    "    - El excel con los links webscrappeados (Parte B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXbrPraa-c9i"
   },
   "source": [
    "#### Ejercicio 1 - Jugando con APIs\n",
    "Usando la API del Series de Tiempo de la Republica Argentina [link](https://datosgobar.github.io/series-tiempo-ar-api/applications/) , obtener dos series de indicadores del Ministerio de Agricultura, Ganadería y Pesca. En total, dicho Ministerio tiene 72 series anuales y pueden buscar dos serie de indicadores de su interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='apis.datos.gob.ar', port=443): Max retries exceeded with url: /series/api/series/?collapse=year&collapse_aggregation=avg&end_date=2016-01-01&ids=155.1_ACULTURSCA_C_0_0_27&limit=5000&start_date=2004-01-01&format=json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001FE3BAB7B50>, 'Connection to apis.datos.gob.ar timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] Se produjo un error durante el intento de conexión ya que la parte conectada no respondió adecuadamente tras un periodo de tiempo, o bien se produjo un error en la conexión establecida ya que el host conectado no ha podido responder",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:492\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    491\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1097\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    610\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    612\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:212\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x000001FE3BAB7B50>, 'Connection to apis.datos.gob.ar timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='apis.datos.gob.ar', port=443): Max retries exceeded with url: /series/api/series/?collapse=year&collapse_aggregation=avg&end_date=2016-01-01&ids=155.1_ACULTURSCA_C_0_0_27&limit=5000&start_date=2004-01-01&format=json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001FE3BAB7B50>, 'Connection to apis.datos.gob.ar timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m### Se realiza la importación de los datos de la primer serie temporal seleccionada, interactuando con la API de datos.gob.ar\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Cada serie cuenta con un ID y una URL detallados en una base dentro de la API para poder realizar las consultas de la misma\u001b[39;00m\n\u001b[0;32m     11\u001b[0m url_arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://apis.datos.gob.ar/series/api/series/?collapse=year&collapse_aggregation=avg&end_date=2016-01-01&ids=155.1_ACULTURSCA_C_0_0_27&limit=5000&start_date=2004-01-01&format=json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url_arg)\n\u001b[0;32m     13\u001b[0m datos \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson() \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Hasta aquí tenemos la serie temporal importada en formato json, a continuación la transformamos en un dataframe de pandas para manipularlo\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:507\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='apis.datos.gob.ar', port=443): Max retries exceeded with url: /series/api/series/?collapse=year&collapse_aggregation=avg&end_date=2016-01-01&ids=155.1_ACULTURSCA_C_0_0_27&limit=5000&start_date=2004-01-01&format=json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001FE3BAB7B50>, 'Connection to apis.datos.gob.ar timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "# Realizamos la importación de todos los paquetes necesarios, previamente instalados\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "\n",
    "### Se realiza la importación de los datos de la primer serie temporal seleccionada, interactuando con la API de datos.gob.ar\n",
    "# Cada serie cuenta con un ID y una URL detallados en una base dentro de la API para poder realizar las consultas de la misma\n",
    "\n",
    "url_arg = \"https://apis.datos.gob.ar/series/api/series/?collapse=year&collapse_aggregation=avg&end_date=2016-01-01&ids=155.1_ACULTURSCA_C_0_0_27&limit=5000&start_date=2004-01-01&format=json\"\n",
    "response = requests.get(url_arg)\n",
    "datos = response.json() \n",
    "print(datos) # Visualizamos los datos obtenidos\n",
    "# Hasta aquí tenemos la serie temporal importada en formato json, a continuación la transformamos en un dataframe de pandas para manipularlo\n",
    "d  = datos['data']\n",
    "data_arg = pd.DataFrame(d)\n",
    "data_arg.columns = ['fecha', 'empleo registrado'] # Renombramos las columnas del dataframe\n",
    "print(data_arg) # Visualizamos el data frame creado\n",
    "\n",
    "# Replicamos los pasos previos para la segunda serie temporal (las variables deben ser nombradas de manera distinta para no pisar los pasos previos) \n",
    "url_arg1 = \"https://apis.datos.gob.ar/series/api/series/?collapse=year&collapse_aggregation=avg&ids=328.2_AGRI_GANADURA__32&limit=5000&format=json\"\n",
    "response1 = requests.get(url_arg1)\n",
    "datos1 = response1.json()\n",
    "print(datos1) # Visualizamos los datos obtenidos\n",
    "d1 = datos1['data']\n",
    "data_arg_1 = pd.DataFrame(d1)\n",
    "data_arg_1.columns = ['fecha', 'remuneracion']\n",
    "print(data_arg_1) # Visualizamos el data frame creado\n",
    "\n",
    "# A continuación realizamos una gráfica exploratoria de los datos obtenidos para ambas series temporales\n",
    "\n",
    "# Primera serie temporal: Empleo registrado del sector privado agricola, ganadero y pesca. Serie de 2004-2016\n",
    "\n",
    "data_arg['fecha'] = pd.to_datetime(data_arg['fecha'])\n",
    "\n",
    "# Creamos la figura y los axes\n",
    "fig, ax = plt.subplots()  # Crear objetos\n",
    "\n",
    "# Definimos series\n",
    "ax.plot(data_arg['fecha'], data_arg['empleo registrado'], label=\"empleo registrado\", color = 'blue')\n",
    "\n",
    "# Modificamos labels y título\n",
    "ax.set_xlabel(\"año\")\n",
    "ax.set_title(\"Figura 1. Empleo registrado del sector privado agricola, ganadero y pesca\\n\" \"Serie de 2004-2016\")\n",
    "\n",
    "# Configuramos las etiquetas del eje X para que solo muestren los meses de enero\n",
    "data_arg_january = data_arg[data_arg['fecha'].dt.month == 1]  # Filtramos solo los meses de enero\n",
    "ax.set_xticks(data_arg_january['fecha'])  # Establecemos los ticks solo en enero\n",
    "ax.set_xticklabels(data_arg_january['fecha'].dt.strftime('%Y-%m'))  # Mostramos solo el año y mes en formato 'YYYY-MM'\n",
    "plt.xticks(rotation=20)\n",
    "\n",
    "# Agregamos leyenda\n",
    "ax.legend()\n",
    "\n",
    "# Mostramos la figura\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Segunda serie temporal: Remuneracion neta de sector argicola, ganadera y caza. Serie de 2004-2016\n",
    "\n",
    "data_arg_1['fecha'] = pd.to_datetime(data_arg_1['fecha'])\n",
    "\n",
    "# Creamos la figura y los axes\n",
    "fig, ax = plt.subplots()  # Crear objetos\n",
    "\n",
    "# Definimos series\n",
    "ax.plot(data_arg_1['fecha'], data_arg_1['remuneracion'], label=\"remuneracion\", color = 'blue')\n",
    "\n",
    "# Modificamos labels y título\n",
    "ax.set_xlabel(\"año\")\n",
    "ax.set_title(\"Figura 2. Remuneracion neta de sector argicola, ganadera y caza\\n\" \"Serie de 2004-2016\")\n",
    "\n",
    "# Configuramos las etiquetas del eje X para que solo muestren los meses de enero\n",
    "data_arg_january = data_arg_1[data_arg['fecha'].dt.month == 1]  # Filtramos solo los meses de enero\n",
    "ax.set_xticks(data_arg_january['fecha'])  # Establecemos los ticks solo en enero\n",
    "ax.set_xticklabels(data_arg_january['fecha'].dt.strftime('%Y-%m'))  # Mostramos solo el año y mes en formato 'YYYY-MM'\n",
    "plt.xticks(rotation=20)\n",
    "\n",
    "# Agregamos leyenda\n",
    "ax.legend()\n",
    "\n",
    "# Mostramos la figura\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2 - Repaso de Pandas\n",
    "Realicen una estadistica descriptiva ambas series de indicadores del Ministerio de Ganaderia y  presente en una tabla en su reporte. Dicha tabla debe mostrar: numero de observaciones, media, desvío standard, min, p25, mediana, p75 y max. Comente la tabla en **el reporte**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_arg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Realizamos la unión de las 2 series temporales en un unico dataframe, a partir de la coincidencia de la columna de fecha entre ambas series\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Lo que realiza el código en este caso es agregar la columna de interes segunda serie temporal (remuneraciones) en el dataframe de empleo, buscando las coincidencias de fecha de la segunda serie con las de la primera (en este caso sabemos que la coincidencia es total)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data_merge \u001b[38;5;241m=\u001b[39m data_arg\u001b[38;5;241m.\u001b[39mmerge(data_arg_1, on\u001b[38;5;241m=\u001b[39mdata_arg_1\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Realizamos la tabla descriptiva unicamente de las 2 variables que interesan del nuevo dataframe fusionado (no la fecha). Redondeamos los decimales a 2\u001b[39;00m\n\u001b[0;32m      6\u001b[0m summary \u001b[38;5;241m=\u001b[39m data_merge[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempleo registrado\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremuneracion\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_arg' is not defined"
     ]
    }
   ],
   "source": [
    "# Realizamos la unión de las 2 series temporales en un unico dataframe, a partir de la coincidencia de la columna de fecha entre ambas series\n",
    "# Lo que realiza el código en este caso es agregar la columna de interes segunda serie temporal (remuneraciones) en el dataframe de empleo, buscando las coincidencias de fecha de la segunda serie con las de la primera (en este caso sabemos que la coincidencia es total)\n",
    "data_merge = data_arg.merge(data_arg_1, on=data_arg_1.columns[0], how='left')\n",
    "print(data_merge) # Visualizamos la fusión\n",
    "\n",
    "# Realizamos la tabla descriptiva unicamente de las 2 variables que interesan del nuevo dataframe fusionado (no la fecha). Redondeamos los decimales a 2\n",
    "summary = data_merge[['empleo registrado', 'remuneracion']].describe().round(2)\n",
    "print(summary) # Visualizamos el resumen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3 - Practicando con Matplotlib\n",
    "Armen dos gráficos de tendencia distintos usando la librería Matplotlib (repasen Clase 3). Uno programandolo con el estilo *pyplot* y otro gráfico de estilo *orientada a objetos*.\n",
    "Recuerde los principios de visualización de datos y comente dichas figuras en su reporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico estilo pyplot\n",
    "\n",
    "# Definimos  vectores  de datos  para  serie 1(empleo registrado)\n",
    "y1 = data_merge[\"empleo registrado\"]\n",
    "x1 = data_merge[\"fecha\"]\n",
    "# Definimos  vectores  de datos  para  serie 2( remuneracion)\n",
    "y2 = data_merge[\"remuneracion\"]\n",
    "x2 = data_merge[\"fecha\"]\n",
    "\n",
    " # Definimos la ruta de la carpeta donde se guardará el gráfico\n",
    "carpeta = '/content/drive/MyDrive/big data/Figura.png'\n",
    "\n",
    "# Verifica si la carpeta existe, si no, créala\n",
    "if not os.path.exists(carpeta):\n",
    "    os.makedirs(carpeta)\n",
    "\n",
    "# Creamos el gráfico: \n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Definimos primer panel\n",
    "plt.subplot(121) # subplot(nrows, ncols, index, **kwargs) donde nrows=1, ncols=2, index=1\n",
    "plt.plot(x1, y1)\n",
    "plt.title(\"A. Empleo registrdo\")\n",
    "plt.grid(True)\n",
    "# Definimos segundo panel\n",
    "plt.subplot(122)\n",
    "plt.plot(x2, y2)\n",
    "plt.title(\"B. Remuneracion neta\")\n",
    "plt.grid(True)\n",
    "# Definimos título general de la figura\n",
    "plt.suptitle(\"Empleo registrado y Remuneracion neta Sector agricola,ganadero y pesca\")\n",
    "\n",
    "# Guardamos el gráfico en la carpeta (no usamos la funcion .show() para ahora guardarla)\n",
    "plt.savefig(os.path.join(carpeta, 'figura_3.png'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico estilo orientado-objetos \n",
    "\n",
    "# Creamos la figura\n",
    "fig, ax = plt.subplots(figsize=(14, 5), ncols=2, nrows=1)\n",
    "\n",
    "# Definimos primer panel\n",
    "\n",
    "   # Definimos series\n",
    "ax[0].plot(x1, y1, label=\"empleo registrado\", color = 'blue')\n",
    "   # Modificamos labels y título\n",
    "ax[0].set_xlabel(\"año\")\n",
    "ax[0].set_ylabel(\"puestos de trabajo\")\n",
    "ax[0].set_title(\"A. Empleo registrado\")\n",
    "  # Agregamos leyenda\n",
    "ax[0].legend()\n",
    "  # Agregamos regilla\n",
    "ax[0].grid(True, linestyle='--', linewidth=0.7, color='gray')\n",
    "  # Agregamos dispercion\n",
    "ax[0].scatter(x1, y1, s=100, c='red', marker='o', alpha=0.7)\n",
    "\n",
    "# Definimos segundo panel\n",
    "\n",
    "   # Definimos series\n",
    "ax[1].plot(x2, y2, label=\"remuneracion\", color = 'orange')\n",
    "   # Modificamos labels y título\n",
    "ax[1].set_xlabel(\"año\")\n",
    "ax[1].set_ylabel(\"pesos\")\n",
    "ax[1].set_title(\"B. Remuneracion neta\")\n",
    "  # Agregamos leyenda\n",
    "ax[1].legend()\n",
    "  # Agregamos regilla\n",
    "ax[1].grid(True, linestyle='--', linewidth=0.7, color='gray')\n",
    "  # Agregamos dispercion\n",
    "ax[1].scatter(x2, y2, s=100, c='blue', marker='o', alpha=0.7)\n",
    "# Definimos título general de la figura\n",
    "fig.suptitle(\"Empleo registrado y Remuneracion neta del Sector agricola,ganadero y pesca\")\n",
    "# Guardamos el gráfico en la carpeta (no usamos la funcion .show() para ahora guardarla)\n",
    "plt.savefig(os.path.join(carpeta, 'figura_4.png'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4 - Documentando noticias locales y en el extranjero\n",
    "Eligan dos sitios web de noticias: uno de Argentina y uno del extranjero (puede ser en castellano o en inglés como el *New York Times*).\n",
    "\n",
    "Utilicen herramientas de webscraping para obtener los **links** de las noticias de la portada en ambos diarios. Guarden cada titular con los links obtenidos en un dataframe. Por lo tanto, debe quedarles un dataframe de 4 columnas: una columna para noticias locales, otra para su link, otra columna de noticias extranjeras y una ultima columna de sus links. Luego, expórtenlo a un archivo de excel.\n",
    "\n",
    "En el reporte este esta parte B:\n",
    "1) Inserten una captura de pantalla de la pagina del diario extranjero y en Argentina de las cuales hacen el ejercicio de extraer titulares y sus links correspondientes. Esto servirá al momento de la corrección para verificar que los links obtenidos hacen referencia a las noticias de ese día y hora.\n",
    "2) Comenten brevemente (1 párrafo) las diferencias/similitudes al extraer titulares en diarios de Argentina versus diario extranjero\n",
    "3) Comenten brevemente (1 párrafo) las dificultades y cómo resolvieron dichas dificultades.\n",
    "\n",
    "*Nota*: es posible que logren obtener los links a las noticias sin el dominio: por ejemplo \"https://www.lanacion.com.ar/\". De ser así, concatenen el dominio a la ruta del link obtenido, tal que se obtenga un link al que se pueda acceder. Es decir, que las cadenas de caracteres finales tendrán la forma: https://www.lanacion.com.ar/*texto_obtenido*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza la importación de los paquetes necesarios\n",
    " \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Se realiza la elección de los sitios web\n",
    "argentina_news_url = 'https://www.clarin.com/' # Clarín in Argentina\n",
    "foreign_news_url = 'https://edition.cnn.com/' # cnn is a well-known international news\n",
    "\n",
    "# Explique elección de sitios web.\n",
    "print(f\"Selected Argentinian news website: {argentina_news_url}\")\n",
    "print(\"Reasoning:Clarín es uno de los periódicos más leídos y respetados de Argentina, y ofrece una cobertura integral de las noticias nacionales..\")\n",
    "print(f\"Selected foreign news website: {foreign_news_url}\")\n",
    "print(\"Reasoning: cnn  es una organización de noticias reconocida mundialmente con una sólida reputación por sus informes objetivos y profundos sobre eventos internacionales..\")\n",
    "\n",
    "# Primero trabajamos con la página web del medio de comunicación local\n",
    "\n",
    "# Importamos el html de la página web de Clarín\n",
    "response = requests.get(argentina_news_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Creamos listas vacias para almacenar los titulares \n",
    "argentina_headlines = []\n",
    "argentina_links = []\n",
    "\n",
    "# Realizamos la importación del código html de\n",
    "\n",
    "# Inspeccione el HTML del sitio web de Clarín para encontrar las etiquetas y atributos relevantes para los titulares y enlaces.\n",
    "# Ejemplo: Suponiendo que los titulares están en etiquetas <h2> dentro de etiquetas <a>\n",
    "# Esta parte podría requerir ajustes según la estructura real del sitio web.\n",
    "for h2_tag in soup.find_all('h2'):\n",
    "    a_tag = h2_tag.find('a')\n",
    "    if a_tag and a_tag.text and a_tag['href']:\n",
    "        argentina_headlines.append(a_tag.text.strip())\n",
    "        argentina_links.append(a_tag['href'])\n",
    "\n",
    "# Muestra los primeros titulares y enlaces extraídos para verificar\n",
    "print(\"First 5 Argentinian Headlines:\")\n",
    "for headline in argentina_headlines[:5]:\n",
    "    print(headline)\n",
    "\n",
    "print(\"\\nFirst 5 Argentinian Links:\")\n",
    "for link in argentina_links[:5]:\n",
    "    print(link)\n",
    "\n",
    "# A continuación trabajamos con la pagina web del medio de comunicación internacional: \n",
    "\n",
    "foreign_news_url = 'https://edition.cnn.com/'\n",
    "response = requests.get(foreign_news_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "foreign_headlines = []\n",
    "foreign_links = []\n",
    "\n",
    "\n",
    "# Inspeccione la estructura HTML de CNN para encontrar titulares y enlaces.\n",
    "# CNN suele usar etiquetas 'span' o 'h3' para los titulares, dentro de etiquetas 'a' para los enlaces, a menudo anidadas en divs con clases específicas.\n",
    "# Necesitamos buscar patrones que contengan de forma fiable tanto el texto del titular como el enlace.\n",
    "\n",
    "# Intento 1: Busque enlaces dentro de clases específicas que se usan a menudo para titulares en CNN.\n",
    "# Tras una inspección rápida (necesaria para mayor precisión), clases como 'container__link' o similares son comunes.\n",
    "for link_tag in soup.find_all('a', class_='container__link'): # Example class, might need adjustment\n",
    "    headline_element = link_tag.find(['span', 'h3']) # Headlines are often in span or h3 within the link\n",
    "    if headline_element and headline_element.text:\n",
    "        headline_text = headline_element.text.strip()\n",
    "        link = link_tag.get('href')\n",
    "\n",
    "        if link and headline_text:\n",
    "            # Ensure the link is absolute\n",
    "            if link.startswith('/'):\n",
    "                foreign_links.append(f\"https://edition.cnn.com{link}\")\n",
    "            else:\n",
    "                foreign_links.append(link)\n",
    "            foreign_headlines.append(headline_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Si el primer intento no arrojó muchos resultados, pruebe con otro enfoque.\n",
    "# Segundo intento: Busque enlaces en las etiquetas \"h2\" o \"h3\", que podrían contener el enlace directamente.\n",
    "if not foreign_headlines:\n",
    "    for heading_tag in soup.find_all(['h2', 'h3']):\n",
    "        a_tag = heading_tag.find('a')\n",
    "        if a_tag and a_tag.text and a_tag['href']:\n",
    "             headline_text = a_tag.text.strip()\n",
    "             link = a_tag.get('href')\n",
    "             if link.startswith('/'):\n",
    "                foreign_links.append(f\"https://edition.cnn.com{link}\")\n",
    "             else:\n",
    "                foreign_links.append(link)\n",
    "             foreign_headlines.append(headline_text)\n",
    "\n",
    "\n",
    "# Imprima el número de titulares extranjeros raspados y los primeros 5 para verificar\n",
    "print(f\"Scraped {len(foreign_headlines)} foreign headlines.\")\n",
    "print(\"First 5 Foreign Headlines:\")\n",
    "for headline in foreign_headlines[:5]:\n",
    "    print(headline)\n",
    "\n",
    "print(\"\\nFirst 5 Foreign Links:\")\n",
    "for link in foreign_links[:5]:\n",
    "    print(link)\n",
    "\n",
    "# A continuación creamos el dataframe con los titulares de las noticias y sus url, tanto del medio local como el internacional\n",
    "\n",
    "# Determinamos la longitud mínima de las listas\n",
    "min_len = min(len(argentina_headlines), len(argentina_links), len(foreign_headlines), len(foreign_links))\n",
    "\n",
    "# Truncamos las listas a la longitud mínima\n",
    "argentina_headlines_truncated = argentina_headlines[:min_len]\n",
    "argentina_links_truncated = argentina_links[:min_len]\n",
    "foreign_headlines_truncated = foreign_headlines[:min_len]\n",
    "foreign_links_truncated = foreign_links[:min_len]\n",
    "\n",
    "# Creamos un diccionario a partir de las listas truncadas\n",
    "data = {\n",
    "    'Noticias Locales': argentina_headlines_truncated,\n",
    "    'Link Local': argentina_links_truncated,\n",
    "    'Noticias Extranjeras': foreign_headlines_truncated,\n",
    "    'Link Extranjero': foreign_links_truncated\n",
    "}\n",
    "\n",
    "# Creamos el DataFrame de pandas a partir del diccionario\n",
    "df_news = pd.DataFrame(data)\n",
    "\n",
    "# Visualizamos el encabezado del DataFrame\n",
    "display(df_news.head())\n",
    "\n",
    "# Finalmente exportamos el dataframe creado a formato excel\n",
    "\n",
    "df_news.to_excel('news_headlines.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "TP1 - Parte 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
